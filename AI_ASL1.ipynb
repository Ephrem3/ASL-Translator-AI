{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "model = load_model(\"model_landmarks.h5\") # Load the model\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(max_num_hands=2, min_detection_confidence=0.5, min_tracking_confidence=0.5) # Initialize the hands module from mediapipe  \n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "alphabet = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\",\n",
    "            \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\",\n",
    "            \"T\", \"U\", \"V\", \"W\", \"X\",  \"Y\", \"Z\"]\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set the main window to fullscreen\n",
    "#cv2.namedWindow('frame', cv2.WND_PROP_FULLSCREEN)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 450)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 450)\n",
    "\n",
    "MAX_LINE_LEN = 10\n",
    "sentence = \"\"\n",
    "word = \"\"\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)      #convert frame to RGB color space because mediapipe works with RGB images\n",
    "    results = hands.process(rgb_frame) \n",
    "\n",
    "    cv2.putText(frame, f\"Word: {word}\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0))\n",
    "\n",
    "    #add text wrapping\n",
    "    lines = []\n",
    "    if sentence.split():\n",
    "        words = sentence.split()\n",
    "        line = words[0]\n",
    "        for word in words[1:]:\n",
    "            if len(line + ' ' + word) <= MAX_LINE_LEN:\n",
    "                line += ' ' + word\n",
    "            else:\n",
    "                lines.append(line)\n",
    "                line = word\n",
    "        lines.append(line)\n",
    "        y_offset = 0\n",
    "        for line in lines:\n",
    "            cv2.putText(frame, f\"Sentence: {line}\", (10, 90 + y_offset), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255))\n",
    "            y_offset += 30\n",
    "    else:\n",
    "        cv2.putText(frame, f\"Sentence: {sentence}\", (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255))\n",
    "\n",
    "    if results.multi_hand_landmarks is not None:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS,  #draw landmarks on frame\n",
    "                                        mp_drawing.DrawingSpec(color=(0, 117, 128), thickness=2, circle_radius=4), \n",
    "                                        mp_drawing.DrawingSpec(color=(53, 101, 77), thickness=2, circle_radius=2)\n",
    "                                        )\n",
    "\n",
    "            landmarks_positions = [(lm.x * frame.shape[1], lm.y * frame.shape[0]) \n",
    "                                    for lm in hand_landmarks.landmark]\n",
    "            landmarks_array = np.array(landmarks_positions).flatten()\n",
    "\n",
    "            bboxC = (\n",
    "                min(landmarks_positions, key=lambda x: x[0])[0],  #letftmost x-coordinate \n",
    "                min(landmarks_positions, key=lambda x: x[1])[1],  #topmost y-coordinate\n",
    "                max(landmarks_positions, key=lambda x: x[0])[0] - min(landmarks_positions, key=lambda x: x[0])[0], #width of bbox\n",
    "                max(landmarks_positions, key=lambda x: x[1])[1] - min(landmarks_positions, key=lambda x: x[1])[1]  #height of bbox\n",
    "            )\n",
    "\n",
    "            #scaling factor to scale the bbox\n",
    "            scaling_factor = 1.5  \n",
    "\n",
    "            #scaled bbox coordinates\n",
    "            bboxC = (\n",
    "                int(bboxC[0] - (bboxC[2] * (scaling_factor - 1) / 2)), #adjsuted leftmost x-coordinate\n",
    "                int(bboxC[1] - (bboxC[3] * (scaling_factor - 1) / 2)), #adjusted topmost y-coordinate\n",
    "                int(bboxC[2] * scaling_factor), #adjusted width of bbox\n",
    "                int(bboxC[3] * scaling_factor)  #adjusted height of bbox\n",
    "            )\n",
    "\n",
    "            for connection in mp_hands.HAND_CONNECTIONS: #draw lines between landmarks\n",
    "                start_point = tuple(np.multiply([hand_landmarks.landmark[connection[0]].x, hand_landmarks.landmark[connection[0]].y], [450, 450]).astype(int))\n",
    "                end_point = tuple(np.multiply([hand_landmarks.landmark[connection[1]].x, hand_landmarks.landmark[connection[1]].y], [450, 450]).astype(int))\n",
    "                cv2.line(rgb_frame, start_point, end_point, (255, 0, 0), 2)  #draw line between two points\n",
    "\n",
    "            cv2.rectangle(frame, (int(bboxC[0]), int(bboxC[1])),\n",
    "                            (int(bboxC[0] + bboxC[2]), int(bboxC[1] + bboxC[3])), (0, 0, 0), 2) #draw rectangle around hand\n",
    "\n",
    "            hand_crop = frame[int(bboxC[1]):int(bboxC[1] + bboxC[3]), int(bboxC[0]):int(bboxC[0] + bboxC[2])] #crop hand from frame\n",
    "            \n",
    "            \n",
    "            landmarks_input = landmarks_array.reshape(1, -1)\n",
    "            \n",
    "            predictions = model.predict(landmarks_input) #predict letter\n",
    "\n",
    "            predicted_class = np.argmax(predictions) #get index of predicted letter\n",
    "\n",
    "            confidence = predictions[0, predicted_class] #get confidence of prediction\n",
    "            \n",
    "            cv2.putText(frame, f\"Predicted: {alphabet[predicted_class]} ({confidence:.2f})\", (10, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)  # Put text on frame\n",
    "            \n",
    "            \n",
    "\n",
    "    cv2.imshow('frame', frame) #show frame\n",
    "\n",
    "    key = cv2.waitKey(1) #wait for key press\n",
    "\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "    elif key == ord('c'):\n",
    "        #cv2.imshow('capture frame', frame)\n",
    "        sentence += alphabet[predicted_class]\n",
    "        word += alphabet[predicted_class]\n",
    "    elif key == ord('d'):\n",
    "        sentence = sentence[:len(sentence) - 1]\n",
    "        word = word[:len(word) - 1]\n",
    "    elif key == ord('s'):\n",
    "        sentence += ' '\n",
    "        word = \"\"\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows() \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
