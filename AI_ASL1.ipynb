{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real-Time Hand Gesture Recogniton and Data Collection\n",
    "\n",
    "1. Introduction \n",
    "\n",
    "This script is designed to capture hand landmarks and images of alphabet hand gestures in real-time using the mediaPipe Hands module. The collected data is organized into directories based on the letters of the alphabet, allowing for the creation of hand gesture datasets for sign langauge.\n",
    "\n",
    "\n",
    "2. Libraries\n",
    "\n",
    "OpenCV(cv2): used for capturing video and image processing\n",
    "mediaPipe(mp): used for hand tracking and landmark detection\n",
    "numpy(np): employed for numerical operations and data handling \n",
    "OS: used for file and directory operation\n",
    "scikit-learn: Utilized for data preprocessing and splitting \n",
    "\n",
    "\n",
    "3. Intitializtaion \n",
    "\n",
    "This script initializes the mediaPipe Hands module and a Video capture object.\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(max_num_hands=2, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 450)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 450)\n",
    "\n",
    "\n",
    "4. Gesture Data Collection \n",
    "\n",
    "This script collects hand landmarks, draws them on the video frame, and saves the landmarks and cropped hand images for each gesture in separate directories.\n",
    "\n",
    "\n",
    "while index < len(alphabet):\n",
    "    # ...\n",
    "    while True:\n",
    "        # ...\n",
    "        if results.multi_hand_landmarks is not None:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # ...\n",
    "                landmarks_positions = [(lm.x * frame.shape[1], lm.y * frame.shape[0]) for lm in hand_landmarks.landmark]\n",
    "                landmarks_file_path = os.path.join(letter_path, letter + str(count) + '.npy')\n",
    "                np.save(landmarks_file_path, landmarks_positions)\n",
    "                # ...\n",
    "                hand_crop = frame[int(bboxC[1]):int(bboxC[1] + bboxC[3]), int(bboxC[0]):int(bboxC[0] + bboxC[2])]\n",
    "                if hand_crop.size != 0:\n",
    "                    if cv2.waitKey(1) & 0xFF == ord('c'):\n",
    "                        image_name = letter + str(count) + '.png'\n",
    "                        image_path = os.path.join(letter_path, image_name)\n",
    "                        cv2.imwrite(image_path, hand_crop)\n",
    "                        # ...\n",
    "                        count += 1\n",
    "        else:\n",
    "            print('no hand')\n",
    "        # ...\n",
    "\n",
    "5. Data Preprocessing \n",
    "\n",
    "Hand gesture images and landmarks are loaded, resized, and normalized for input into the gesture classification model.\n",
    "\n",
    "for letter in alphabet:\n",
    "    letter_path = os.path.join(data_path, letter)\n",
    "    for file_name in os.listdir(letter_path):\n",
    "        # ...\n",
    "        hand_landmarks = np.load(landmarks_path, allow_pickle=True)\n",
    "        image = cv2.imread(image_path)\n",
    "        # ...\n",
    "        landmarks_positions = hand_landmarks.flatten()\n",
    "        images.append(hand_crop_array)\n",
    "        landmarks_list.append(landmarks_positions)\n",
    "        labels.append(letter)\n",
    "        # ...\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "landmarks = np.array(landmarks_list)\n",
    "\n",
    "\n",
    "\n",
    "6. Gesture Classification Model \n",
    "\n",
    "A simple fully connected neural network is built and trained to predict the hand gesture from the landmarks.\n",
    "\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Flatten(input_shape=(42,)))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(len(alphabet), activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "7. Model Training and Evaluation \n",
    "\n",
    "The model is trained and evaluated on the dataset split into training and testing sets.\n",
    "\n",
    "history = model.fit(\n",
    "    x=landmarks_train,\n",
    "    y=y_train,\n",
    "    epochs=32,\n",
    "    batch_size=64,\n",
    "    validation_data=(landmarks_test, y_test)\n",
    ")\n",
    "\n",
    "test_loss, test_acc = model.evaluate(landmarks_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc}\")\n",
    "\n",
    "\n",
    "8. Gesture Recognition\n",
    "\n",
    "Real-time handlandmarks are predicted using the trained model, and the predicted gesture is displayed on the feed.\n",
    "  \n",
    "while index < len(alphabet):\n",
    "    # ...\n",
    "    while True:\n",
    "        # ...\n",
    "        if results.multi_hand_landmarks is not None:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # ...\n",
    "                landmarks_positions = [(lm.x * frame.shape[1], lm.y * frame.shape[0]) for lm in hand_landmarks.landmark]\n",
    "                landmarks_array = np.array(landmarks_positions).flatten()\n",
    "                # ...\n",
    "                predictions = model.predict(landmarks_array.reshape(1, -1))\n",
    "                predicted_class = np.argmax(predictions)\n",
    "                cv2.putText(frame, f\"Predicted: {alphabet[predicted_class]}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        # ...\n",
    "\n",
    "\n",
    "        Conclusion \n",
    "\n",
    "        This script provides a comprehensive solution for real-time hand gesture recognition making it a valuable tool for application such as sign language translation.\n",
    "\n",
    "\n",
    "Sources:\n",
    "\n",
    "1. https://medium.com/mlearning-ai/american-sign-language-alphabet-recognition-ec286915df12\n",
    "2. https://www.mdpi.com/1424-8220/23/18/7970\n",
    "3. https://www.kaggle.com/datasets/grassknoted/asl-alphabet\n",
    "4. https://github.com/computervisioneng/...\n",
    "   #computervision #signlanguagedetection #objectdetection #scikitlearn #python #opencv #mediapipe #landmarkdetection\n",
    "5. https://github.com/yuliianikolaenko/asl-alphabet-classification\n",
    "6. https://github.com/topics/asl-recognizer\n",
    "7. https://github.com/topics/asl-alphabet-translator\n",
    "8. https://github.com/11a55an/american-sign-language-detection\n",
    "9. https://github.com/VedantMistry13/American-Sign-Language-Recognition-using-Deep-Neural-Network\n",
    "10. https://github.com/kinivi/hand-gesture-recognition-mediapipe/blob/main/app.py\n",
    "11. https://github.com/Kazuhito00/hand-ge...\n",
    "12. https://www.computervision.zone/cours...\n",
    "13. https://github.com/nicknochnack/Actio...,  Complete Machine Learning and Data Science Courses\n",
    "14. chatgpt\n",
    "15. https://github.com/ivangrov\n",
    "16. https://www.youtube.com/channel/UCxladMszXan-jfgzyeIMyvw/about\n",
    "17. https://github.com/nicknochnack/Actio...\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. collecting keypoints for training and testing \n",
    "2. preprocessing data \n",
    "3. build a model and train\n",
    "4. test predictions \n",
    "5. evaluation using confusion matrix and accuracy\n",
    "6. test in real-time \n",
    "7. Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#set up mediapipe\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(max_num_hands=2, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "#set up data path, define alphabet\n",
    "data_path = \"/Users/reagan/desktop/AI/AI_ASL/\"\n",
    "alphabet = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \n",
    "            \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\",  \"Y\", \"Z\"]\n",
    "\n",
    "#set up camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 450)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 450)\n",
    "\n",
    "index = 0\n",
    "\n",
    "#loop through alphabet\n",
    "while index < len(alphabet):\n",
    "    letter = alphabet[index]\n",
    "    letter_path = os.path.join(data_path, letter)\n",
    "    os.makedirs(letter_path, exist_ok=True)\n",
    "    \n",
    "    print(letter, letter_path)\n",
    "    \n",
    "    #set up count for images\n",
    "    count = 0\n",
    "\n",
    "    #main loop for collecting images\n",
    "    while True:\n",
    "        ret, frame = cap.read()   \n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) #convert to RGB for mediapipe\n",
    "        results = hands.process(rgb_frame)\n",
    "\n",
    "        if results.multi_hand_landmarks is not None:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS,\n",
    "                                          mp_drawing.DrawingSpec(color=(0, 117, 128), thickness=2, circle_radius=4),\n",
    "                                          mp_drawing.DrawingSpec(color=(53, 101, 77), thickness=2, circle_radius=2)\n",
    "                                          )\n",
    "                \n",
    "                # Extract landmark positions and save them to a file\n",
    "                landmarks_positions = [(lm.x * frame.shape[1], lm.y * frame.shape[0]) for lm in hand_landmarks.landmark]\n",
    "                landmarks_file_path = os.path.join(letter_path, letter + str(count) + '.npy')\n",
    "                np.save(landmarks_file_path, landmarks_positions)\n",
    "\n",
    "                # Extract bounding box of hand and draw it on the frame\n",
    "                bboxC = (\n",
    "                    min(landmarks_positions, key=lambda x: x[0])[0],\n",
    "                    min(landmarks_positions, key=lambda x: x[1])[1],\n",
    "                    max(landmarks_positions, key=lambda x: x[0])[0] - min(landmarks_positions, key=lambda x: x[0])[0],\n",
    "                    max(landmarks_positions, key=lambda x: x[1])[1] - min(landmarks_positions, key=lambda x: x[1])[1]\n",
    "                )\n",
    "                scaling_factor = 1.5 # bounding box size multiplier to get padding around the hand\n",
    "                bboxC = (\n",
    "                    int(bboxC[0] - (bboxC[2] * (scaling_factor - 1) / 2)), # adjusted left coordinate\n",
    "                    int(bboxC[1] - (bboxC[3] * (scaling_factor - 1) / 2)), # adjusted right coordinate\n",
    "                    int(bboxC[2] * scaling_factor), # adjusted width\n",
    "                    int(bboxC[3] * scaling_factor)  # adjusted height\n",
    "                )\n",
    "\n",
    "                # Draws lines between hand landmarks \n",
    "                for connection in mp_hands.HAND_CONNECTIONS:\n",
    "                    start_point = tuple(np.multiply([hand_landmarks.landmark[connection[0]].x, hand_landmarks.landmark[connection[0]].y], [450, 450]).astype(int))\n",
    "                    end_point = tuple(np.multiply([hand_landmarks.landmark[connection[1]].x, hand_landmarks.landmark[connection[1]].y], [450, 450]).astype(int))\n",
    "                    cv2.line(rgb_frame, start_point, end_point, (255, 0, 0), 2)\n",
    "                \n",
    "                # Draw rectangle around hand using adjusted bounding box\n",
    "                cv2.rectangle(frame, (int(bboxC[0]), int(bboxC[1])),\n",
    "                              (int(bboxC[0] + bboxC[2]), int(bboxC[1] + bboxC[3])), (0, 0, 0), 2)\n",
    "\n",
    "                \n",
    "\n",
    "                # Crop hand from frame using adjusted bounding box\n",
    "                hand_crop = frame[int(bboxC[1]):int(bboxC[1] + bboxC[3]), int(bboxC[0]):int(bboxC[0] + bboxC[2])]\n",
    "                if hand_crop.size != 0:\n",
    "                    if cv2.waitKey(1) & 0xFF == ord('c'):\n",
    "                        image_name = letter + str(count) + '.png'\n",
    "                        image_path = os.path.join(letter_path, image_name)\n",
    "                        cv2.imwrite(image_path, hand_crop)\n",
    "                        print('image_name:', image_name)\n",
    "                        count += 1\n",
    "                \n",
    "                print('hand')\n",
    "\n",
    "        else:\n",
    "            print('no hand')\n",
    "\n",
    "        cv2.imshow('frame', frame)\n",
    "\n",
    "        key = cv2.waitKey(1)\n",
    "\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "        elif key == ord(' '):\n",
    "            count = 0\n",
    "            break\n",
    "\n",
    "    index += 1\n",
    "\n",
    "print(count)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# set up mediapipe\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(max_num_hands=2, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# set up data path and alphabet\n",
    "data_path = \"/Users/reagan/desktop/AI/AI_ASL/\"\n",
    "alphabet = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\",\n",
    "            \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\",\n",
    "            \"T\", \"U\", \"V\", \"W\", \"X\",  \"Y\", \"Z\"]\n",
    "\n",
    "# lists for storing data\n",
    "images = []\n",
    "labels = []\n",
    "landmarks_list = []\n",
    "\n",
    "# initialize LabelEncoder\n",
    "label_encoder = LabelEncoder() \n",
    "\n",
    "# loop through each letter in alphabet and each file in each letter folder\n",
    "for letter in alphabet:  \n",
    "    letter_path = os.path.join(data_path, letter) \n",
    "    for file_name in os.listdir(letter_path):\n",
    "        image_path = os.path.join(letter_path, file_name)\n",
    "        landmarks_path = os.path.join(letter_path, file_name.replace(\".png\", \".npy\")) # replace image extension with .npy extension\n",
    "        print(image_path)\n",
    "        print(landmarks_path)\n",
    "\n",
    "        # load image and landmarks\n",
    "\n",
    "        hand_landmarks = np.load(landmarks_path) # load landmarks\n",
    "\n",
    "        image = cv2.imread(image_path) # load image\n",
    "\n",
    "        if image is not None:\n",
    "            rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "         # resize, normalize,and flatten image\n",
    "            hand_crop_resized = cv2.resize(hand_crop, (224, 224))\n",
    "            hand_crop_resized = hand_crop_resized.astype(float) / 255.0\n",
    "            hand_crop_array = img_to_array(hand_crop_resized)\n",
    "            landmarks_positions = hand_landmarks.flatten()\n",
    "\n",
    "            print(landmarks_positions.shape)\n",
    "            print(hand_crop_array.shape)\n",
    "            \n",
    "            images.append(hand_crop_array)\n",
    "            landmarks_list.append(landmarks_positions)\n",
    "            labels.append(letter)\n",
    "\n",
    "            # save landmarks as .npy file\n",
    "            landmarks_array = np.array(landmarks_positions)\n",
    "            landmarks_save_path = os.path.join(letter_path, file_name.replace(\".png\", \".npy\"))\n",
    "            np.save(landmarks_save_path, landmarks_array)\n",
    "\n",
    "# encode labels and convert to categorical\n",
    "\n",
    "labels = label_encoder.fit_transform(labels)\n",
    "labels = to_categorical(labels, num_classes=len(alphabet))\n",
    "\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "landmarks = np.array(landmarks_list)\n",
    "\n",
    "# split data into training and testing sets\n",
    "\n",
    "if len(landmarks) > 0:\n",
    "    X_train, X_test, landmarks_train, landmarks_test, y_train, y_test = train_test_split(\n",
    "        images, landmarks, labels, test_size=0.2, random_state=42) # split images, landmarks, and labels\n",
    "\n",
    "        \n",
    "    print(X_train.shape)\n",
    "    print(X_test.shape)\n",
    "    print(y_train.shape)\n",
    "    print(y_test.shape)\n",
    "    print(landmarks_train.shape)\n",
    "    print(landmarks_test.shape)\n",
    "    \n",
    "else:\n",
    "    print(\"No data available for splitting.\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = models.Sequential()\n",
    "#flatten the input\n",
    "model.add(layers.Flatten(input_shape=(42,)))  # 42 = 21 landmarks * 2 coordinates\n",
    "model.add(layers.Dense(128, activation='relu')) # 128 = 2^7 nuerons in hidden layer\n",
    "model.add(layers.Dropout(0.5)) #  50% dropout rate to prevent overfitting\n",
    "model.add(layers.Dense(len(alphabet), activation='softmax'))# 26 neurons in output layer for 26 classes\n",
    "\n",
    "\n",
    "#comile the model\n",
    "#adam optimizer is used to minimize the loss function by updating the weights for each epoch\n",
    "#loss function is categorical crossentropy because there are more than 2 classes\n",
    "#accuracy is used to measure the performance of the model\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#train the model\n",
    "history = model.fit( \n",
    "    x=landmarks_train,     #input data\n",
    "    y=y_train,            #output data (labels)\n",
    "    epochs=32,            #32 iterations\n",
    "    batch_size=64,        #number of samples per gradient update\n",
    "    validation_data=(landmarks_test, y_test) #data to validate the model on\n",
    ")\n",
    "\n",
    "\n",
    "test_loss, test_acc = model.evaluate(landmarks_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc}\")\n",
    "\n",
    "\n",
    "model.save(\"/Users/reagan/desktop/model_landmarks.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plot the accuracy and loss for the training and validation data\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy') \n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_scores = cross_val_score(model, landmarks, labels, cv=5, scoring='accuracy')\n",
    "print(\"c.v\", cv_scores)\n",
    "print(\"mean_accuracy\", np.mean(cv_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-06 05:56:25.307934: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1701860190.028681       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-5.1.35), renderer: AMD Radeon Pro 555X OpenGL Engine\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "2023-12-06 05:56:30.308 Python[7726:899894] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A /Users/reagan/desktop/AI/AI_ASL/A\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.1) /Users/runner/work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m/Ephrem3/ASL-Translator-AI/AI_ASL1.ipynb Cell 8\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://github%2B7b2276223a312c22726566223a7b2274797065223a342c226964223a22657069632d332d65706872656d227d7d/Ephrem3/ASL-Translator-AI/AI_ASL1.ipynb#X25sdnNjb2RlLXZmcw%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://github%2B7b2276223a312c22726566223a7b2274797065223a342c226964223a22657069632d332d65706872656d227d7d/Ephrem3/ASL-Translator-AI/AI_ASL1.ipynb#X25sdnNjb2RlLXZmcw%3D%3D?line=34'>35</a>\u001b[0m     ret, frame \u001b[39m=\u001b[39m cap\u001b[39m.\u001b[39mread()\n\u001b[0;32m---> <a href='vscode-notebook-cell://github%2B7b2276223a312c22726566223a7b2274797065223a342c226964223a22657069632d332d65706872656d227d7d/Ephrem3/ASL-Translator-AI/AI_ASL1.ipynb#X25sdnNjb2RlLXZmcw%3D%3D?line=35'>36</a>\u001b[0m     rgb_frame \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mcvtColor(frame, cv2\u001b[39m.\u001b[39;49mCOLOR_BGR2RGB)      \u001b[39m#convert frame to RGB color space because mediapipe works with RGB images\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://github%2B7b2276223a312c22726566223a7b2274797065223a342c226964223a22657069632d332d65706872656d227d7d/Ephrem3/ASL-Translator-AI/AI_ASL1.ipynb#X25sdnNjb2RlLXZmcw%3D%3D?line=36'>37</a>\u001b[0m     results \u001b[39m=\u001b[39m hands\u001b[39m.\u001b[39mprocess(rgb_frame) \n\u001b[1;32m     <a href='vscode-notebook-cell://github%2B7b2276223a312c22726566223a7b2274797065223a342c226964223a22657069632d332d65706872656d227d7d/Ephrem3/ASL-Translator-AI/AI_ASL1.ipynb#X25sdnNjb2RlLXZmcw%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39mif\u001b[39;00m results\u001b[39m.\u001b[39mmulti_hand_landmarks \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.8.1) /Users/runner/work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "model = load_model(\"/Users/reagan/models/model_landmarks.h5\") # Load the model\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(max_num_hands=2, min_detection_confidence=0.5, min_tracking_confidence=0.5) # Initialize the hands module from mediapipe  \n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "data_path = \"/Users/reagan/desktop/AI/AI_ASL/\" # Path to the data\n",
    "alphabet = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\",\n",
    "            \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\",\n",
    "            \"T\", \"U\", \"V\", \"W\", \"X\",  \"Y\", \"Z\"]\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 450)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 450)\n",
    "\n",
    "index = 0\n",
    "\n",
    "while index < len(alphabet): # Loop through each letter\n",
    "    letter = alphabet[index]\n",
    "    letter_path = os.path.join(data_path, letter)\n",
    "    os.makedirs(letter_path, exist_ok=True)\n",
    "    \n",
    "    print(letter, letter_path)\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)      #convert frame to RGB color space because mediapipe works with RGB images\n",
    "        results = hands.process(rgb_frame) \n",
    "\n",
    "        if results.multi_hand_landmarks is not None:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS,  #draw landmarks on frame\n",
    "                                          mp_drawing.DrawingSpec(color=(0, 117, 128), thickness=2, circle_radius=4), \n",
    "                                          mp_drawing.DrawingSpec(color=(53, 101, 77), thickness=2, circle_radius=2)\n",
    "                                          )\n",
    "\n",
    "                landmarks_positions = [(lm.x * frame.shape[1], lm.y * frame.shape[0]) \n",
    "                                       for lm in hand_landmarks.landmark]\n",
    "                landmarks_array = np.array(landmarks_positions).flatten()\n",
    "\n",
    "                bboxC = (\n",
    "                    min(landmarks_positions, key=lambda x: x[0])[0],  #letftmost x-coordinate \n",
    "                    min(landmarks_positions, key=lambda x: x[1])[1],  #topmost y-coordinate\n",
    "                    max(landmarks_positions, key=lambda x: x[0])[0] - min(landmarks_positions, key=lambda x: x[0])[0], #width of bbox\n",
    "                    max(landmarks_positions, key=lambda x: x[1])[1] - min(landmarks_positions, key=lambda x: x[1])[1]  #height of bbox\n",
    "                )\n",
    "\n",
    "                #scaling factor to scale the bbox\n",
    "                scaling_factor = 1.5  \n",
    "\n",
    "                #scaled bbox coordinates\n",
    "                bboxC = (\n",
    "                    int(bboxC[0] - (bboxC[2] * (scaling_factor - 1) / 2)), #adjsuted leftmost x-coordinate\n",
    "                    int(bboxC[1] - (bboxC[3] * (scaling_factor - 1) / 2)), #adjusted topmost y-coordinate\n",
    "                    int(bboxC[2] * scaling_factor), #adjusted width of bbox\n",
    "                    int(bboxC[3] * scaling_factor)  #adjusted height of bbox\n",
    "                )\n",
    "\n",
    "                for connection in mp_hands.HAND_CONNECTIONS: #draw lines between landmarks\n",
    "                    start_point = tuple(np.multiply([hand_landmarks.landmark[connection[0]].x, hand_landmarks.landmark[connection[0]].y], [450, 450]).astype(int))\n",
    "                    end_point = tuple(np.multiply([hand_landmarks.landmark[connection[1]].x, hand_landmarks.landmark[connection[1]].y], [450, 450]).astype(int))\n",
    "                    cv2.line(rgb_frame, start_point, end_point, (255, 0, 0), 2)  #draw line between two points\n",
    "\n",
    "                cv2.rectangle(frame, (int(bboxC[0]), int(bboxC[1])),\n",
    "                              (int(bboxC[0] + bboxC[2]), int(bboxC[1] + bboxC[3])), (0, 0, 0), 2) #draw rectangle around hand\n",
    "\n",
    "                hand_crop = frame[int(bboxC[1]):int(bboxC[1] + bboxC[3]), int(bboxC[0]):int(bboxC[0] + bboxC[2])] #crop hand from frame\n",
    "                if hand_crop.size != 0:\n",
    "                    if cv2.waitKey(1) & 0xFF == ord('c'):\n",
    "                        image_name = letter + str(count) + '.png'\n",
    "                        image_path = os.path.join(letter_path, image_name)\n",
    "                        cv2.imwrite(image_path, hand_crop)\n",
    "                        print('image_name:', image_name)\n",
    "                        count += 1\n",
    "                \n",
    "                landmarks_input = landmarks_array.reshape(1, -1)\n",
    "               \n",
    "                predictions = model.predict(landmarks_input) #predict letter\n",
    "\n",
    "                confidence = predictions[0, predicted_class] #get confidence of prediction\n",
    "                \n",
    "                predicted_class = np.argmax(predictions) #get index of predicted letter\n",
    "                cv2.putText(frame, f\"Predicted: {alphabet[predicted_class]} ({confidence:.2f})\", (10, 30),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)  # Put text on frame\n",
    "\n",
    "        cv2.imshow('frame', frame) #show frame\n",
    "\n",
    "        key = cv2.waitKey(1) #wait for key press\n",
    "\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "     #   elif key == ord(' '):\n",
    "    #      count = 0\n",
    "     #       break\n",
    "\n",
    "    index += 1\n",
    "\n",
    "print(count)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows() \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
